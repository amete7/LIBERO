policy_type: SkillGPT_Model
extra_num_layers: 0
extra_hidden_size: 32
extra_embedding_size: 32 # joint_states emb size is twice this value
obs_emb_dim: 256 # from 512 to this value using MLP
cat_obs_dim: 640 # 256 + 256 + 32 + 32 + 64 (front_cam, gripper_cam, gripper_state (2), gripper_pos (3), joint_states (7))
action_dim: 7

prior:
    start_token: 33
    offset_layers: 0
    offset_hidden_dim: 512

    vocab_size: [9,9,15]
    block_size: 4
    
    n_layer: 6
    n_head: 6
    n_embd: 384
    attn_pdrop: 0.1
    embd_pdrop: 0.1
    beam_size: 5 # value of k for top k sampling
    temperature: 1.0 # temperature for sampling

skill_vae:
    path: null
    skill_block_size: 96

channel: 16
codebook_dim: 256 # 512 for vq or 256 for fsq
kernels: [5,3] # first 5 rest 3
strides: [2,2,2,3] # conv module will have 3 layers with strides 2,2,1

quantizer_args:
    quantizer_type: "residual_fsq" # "vq" or "fsq" or residual_fsq or residual_vq
    dim: 256
    codebook_size: [8,8,16] # 1024 for vq or [8,8,16] for residual_vq | only used for vq
    num_quantizers: 3
    levels: [[3,3],[3,3],[5,3]] # [8,5,5,5] for fsq or [[3,3],[3,3],[5,3]] for residual_fsq | only used for fsq

offset_loss_scale: 10.0
lang_emb_dim: 512 # clip embedding size
mpc_horizon: 24 # mpc horizon for execution

defaults:
    - data_augmentation@color_aug: batch_wise_img_color_jitter_group_aug.yaml
    - data_augmentation@translation_aug: translation_aug.yaml
    - image_encoder: resnet_encoder.yaml
    - language_encoder: mlp_encoder.yaml