policy_type: SkillGPT_Model
extra_num_layers: 0
extra_hidden_size: 32
extra_embedding_size: 32 # joint_states emb size is twice this value

cross_z: true # decoder cross attends to z
use_m4: 1 # when m4 is set to 1, the decoder also cross attends to observation
action_dim: 7
obs_emb_dim: 256 # from 512 to this value using MLP
cat_obs_dim: 640 # 256 + 256 + 32 + 32 + 64 (front_cam, gripper_cam, gripper_state (2), gripper_pos (3), joint_states (7))
encoder_dim: 256
decoder_dim: 256
skill_block_size: 32 # this is input sequence length to encoder

encoder_heads: 4
encoder_layers: 4
decoder_heads: 4
decoder_layers: 4

resid_pdrop: 0.1
attn_pdrop: 0.1
use_causal_encoder: true
use_causal_decoder: true

vq_type: "fsq" # "vq" or "fsq"
fsq_level: [8,5,5,5]
codebook_dim: 512 # only used for vq
codebook_size: 1024 # only used for vq

kernel_sizes: [5,3,3] # conv module will have 3 layers with kernel sizes 5,3,3
strides: [2,2,1] # conv module will have 3 layers with strides 2,2,1

prior:
    vocab_size: 1004
    block_size: 8 # since we have 8 z-tokens corresponding to 32 actions
    output_dim: 1000
    start_token: 1001
    n_layer: 8
    n_head: 8
    n_embd: 512
    beam_size: 5 # value of k for top k sampling
    temperature: 1.0 # temperature for sampling

offset_loss_scale: 1
lang_emb_dim: 512 # clip embedding size
mpc_horizon: 16 # mpc horizon for execution

defaults:
    - data_augmentation@color_aug: batch_wise_img_color_jitter_group_aug.yaml
    - data_augmentation@translation_aug: translation_aug.yaml
    - image_encoder: resnet_encoder.yaml
    - language_encoder: mlp_encoder.yaml
