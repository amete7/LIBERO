# @package _global_

defaults:
  - data: default
  - policy: skill_GPT
  - train: default
  - eval: default
  - lifelong: multitask
  - test: null
  - _self_

# change defaults
data:
  seq_len: 32
  obs_seq_len: 1
eval:
  eval: true
lifelong:
  eval_in_train: true
train:
  batch_size: 256
  num_workers: 6
  n_epochs: 100
policy:
  use_m4: 0
  cross_z: false
  vq_type: "fsq"
  fsq_level: [8,5,5,5]
  use_causal_decoder: false
  strides: [1,1,1]

pretrain_skillVAE_path: "/satassdscratch/amete7/LIBERO/experiments_clip/LIBERO_90/Multitask_Pretrain/SkillVAE_Model/ResnetEncoder/m3co_32_f4_k3s1_tf/run_001/multitask_model_ep40.pth"
exp_name: "trial" #"m3co_32_f4_k3s1_ff"
seed: 10000
use_wandb: false
wandb_project: "diff-skill-libero"
folder: null # use default path
bddl_folder: null # use default path
init_states_folder: null # use default path
load_previous_model: false
device: "cuda"
task_embedding_format: "clip"
task_embedding_one_hot_offset: 1
pretrain: false
pretrain_model_path: ""
benchmark_name: "LIBERO_90"
